\section{Methods of Inquiry and Analysis}

\subsection{The architecture of our method}

Figure~\ref{fig:architecture} depicts the overall architecture of our method. It consists of four main parts:
\begin{enumerate}
\item aaaa 
  Real-time streaming: several frameworks exist for real-time streaming. Two of the most notable ones are Apache Storm and Apache Spark, which offer distributed real-time streaming and processing capabilities. Spark allows small-scale batching while Storm is built on a one-at-a-time processing model, where a datum is processed as it arrives. Input streams will be streamed by specific modules then processed (analysed) by other dedicated modules. For instance, in the case of Storm, a topology has to be built to stream news, photos and videos from a set of predefined sources such as Twitter, Kafka, and Flume. Storm consists of \emph{spouts} and \emph{bolts}. Spout is the source of streams and is responsible for consuming the data from the specified source, mainly for reading tuples from an external source and emitting them to spouts. Bolts are responsible for the processing that includes filtering, functions, aggregations, joins and interaction with databases. Real-time streaming will be an essential component of our architecture, considering the time-critical nature of the learning process targeting humanitarian conditions on the ground.

\item Analytics and credibility analysis: The output of the streaming phase will be directed to another analytics framework, such as Spark, for further analysis (e.g., credibility analysis). Spark is a fast and general engine for big data processing. Moreover, Spark provides machine learning primitives through its MLLib library. MLlib contains many learning algorithms and support utilities, including logistic and linear regression, classification, decision trees, clustering, distributed linear algebra, and statistics. Spark also provides for efficient distributed graph processing algorithms through the GraphX library that can be applied on the streams to build clusters for credibility analysis as well. 

\item Profiling: The output of some bolts and of Spark will be stored in a non-relational database such as HBase that will be used to validate and annotate our analyser. HBase is a NoSQL database. It is an open source implementation of Big Table \cite{ChangDGHWBCFG08}, which was proposed by Google to handle big data using distributed its file system storage, the Google
File System \cite{GhemawatGL03}, in a well-formed structure. 

\item Visualisation mapping credible news to location: the output of Spark will be redirected to maps and dashboards that mimic critical stories in real-time. Maps can be implemented using open-source libraries such as Leaflet~\cite{leafleft}, which is an open-source JavaScript library for interactive maps. One can also consider Shiny, a web application framework for R. Additionally, we consider to support other visualization techniques such as dashboards to simplify complex statistics resulting from the analysis phase. 

\end{enumerate}

\begin{figure}
\centering
%\includegraphics[scale=0.5]{fig/architecture1-crop}
\caption{Overall Architecture.}
\label{fig:architecture}
\end{figure}

We dedicate the remainder of this section to further discussion on data acquisition, wrangling, and analysis. 


\subsection{Big Data and Wide Data} 

In our proposed work, we will be collecting Data in English and Arabic, pulling it from different sources, such as social networks, news articles, and blogs. Some reports/data will originate from Facebook and Twitter users, as opposed to when users share sources from news or political organizations on those platforms. Our data will also be in different formats, such as text, videos, and MP3s. It will be flowing in daily at a hefty rate. As such, we claim that this data will be both ``big'' and ``wide''. Big data refers to data that has large volume, with high velocity (which refers to the the speed at which data is created). Both aspects are often very problematic. Our architecture proposed above is intended to address both the volume as well as the velocity of the data we expect to retrieve from online resources. In contrast, wide data is one that is pulled from disparate sources, hence the name. In our case, we are faced with a diversity of news sources that are sometimes even tied to third parties. Without an attempt to correct the discrepancies, the variety of data formats with no common identifier will make it initially largely inconsistent, which makes correlations impossible to draw. Wide data must be cleansed by removing duplicates and accounting for missing information, as well as modelled in a meaningful form. This does seem like an enormous task at stake, but wide availability of machine learning technology in application to natural language processing transforms inaccurate data into fully understandable data, and goes further to enable us to adapt to new types of irregularities.

To do this, we draw on expertise in computational linguistics to incorporate natural language processing (NLP) tools that can be applied on the streamed data, particularly, for tasks such as automated syntactic and semantic translation.  Syntactic translation through machine translation models will be sought to resolve duplication and inconsistencies amongst articles in the same language. Semantic translation will be sought to resolve duplication and inconsistencies amongst articles in different languages. For instance, the Stanford POS tagger~\cite{toutanova1,toutanova2} can be used to assign parts of speech to each word (and other tokens), such as nouns, verbs, and adjectives, to an input stream. The Named Entity Recognition (NER) Classifier~\cite{FinkelGM05} can be used to extract all named entities (e.g., location, organisation, etc.,) from an input stream. New techniques of auto-translate with semantic analysis can now cope with not only the structure of the data, but the meaning, even in multiple languages.


\subsection{Machine Learning: Modeling and Validation} 

Our learning cycle can be described as such: (1) descriptive analytics: what really happened so far in the context of dishonest media reporting on Syria? Here, we will be relying on previous fact-checking studies of news reports or carrying out the investigations ourselves. (2) diagnostic analytics: why did it happen? Here, we conduct an in-depth study exploring all the possible correlations among critical features affecting news honesty. And (3) prescriptive analytics: how can we prevent it from happening again? it is here that the machine learning approaches suggested below will be of utmost relevance. In our analysis, we will be targeting questions related such as, in which location(s) (1) are people being taken as human shields? (2) are chemical weapons being used? (3) are living conditions deteriorating? (4) are there significantly more civilian casualties than military? (5) are communities under a siege approaching the brinks? (6) are hospitals, aid convoys, and other relief institutions being targeted? (7) are bombings of civilian infrastructure blamed on bad information? Our final decisions will be based on the data obtained and the rules we will be deriving by identifying correlations and relative causal relationships using a variety of approaches we investigate below. 

\subsubsection{Semi-supervised learning}

We will be adopting a semi-supervised approach to our problem. Semi-supervised learning is a class of supervised learning techniques that also make use of unlabeled data for training -- typically a small amount of labeled data with a large amount of unlabeled data. The acquisition of labeled data for a learning problem often requires a skilled human agent. The cost associated with the labeling process thus may render a fully labeled training set infeasible, whereas acquisition of unlabeled data is relatively inexpensive. On the other hand, labeling data can yield significant insight into the associated features. To combine the best of the two approaches, semi-supervised learning can be of great practical value. Particularly, unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. 
 
\subsubsection{Bias versus dishonesty}
 
We shall begin by identifying features that may contribute to bias in the Syrian conflict, which can be related to ideology, or geopolitical considerations. In this step, we draw on expertise in media studies, to help us re-examine the available literature on media bias and propose ways by which this can be adapated to the Syrian conflict. We will aim to quantify the degree to which media outlets exhibit media bias using recent approaches akin to \cite{NSZML15}, but again, adapted for the Syrian conflict. The approach cited encodes bias patterns in a low-rank space which corresponds to a latent media bias space that aligns well with political ideology and outlet type. A linguistic analysis
exposes striking differences across the latent dimensions, showing how the different types of media outlets portray different realities even when reporting on the same events.

By the end of this phase, news can be filtered based on bias, and sources exhibiting low bias receive a higher honesty score. Thereafter, we shift our attention to the criteria that predicts the likelihood that a biased piece of news is dishonest. 
In this step, we also draw on expertise in media studies, to help develop mechanisms by which news articles can flagrantly be cast as dishonest. To this end, we shall develop the notion of ``reputation'' based on a variety of features. Including exhibited bias, a source's reputation should primarily also be affected by past performances, where a certain weight is assigned depending on the percentage of dishonest news it has reported in the span of a given number of years. Whilst this gives an initial estimate for the reputation of a given source, further refinements based on machine learning techniques should help tune this score even further.

\subsubsection{Potential learning techniques for predicting accurate reputations scores}

A primary question the authors need to answer is whether the learning model should be based on a time series regression model or normal regression model. Time series regression accounts for the autocorrelation between time events, which always exists. In an autocorrelation, the next value in a time series is affected by earlier values in the time series. Intuitively, this is equivalent to asking whether the reputation of a given source is seasonal, periodic, depending on political factors, say, or changes on the political map. In contrast, in a normal regression, independence of serial errors are presumed, or at least minimized. 

Other relevant algorithms we need to assess for accuracy in the context of our problem are the following. At the supervised level, the $k$-nearest neighbour approach may be relevant in the sense that dishonest sources on the ground which may be close enough geographically and not separated by some ideological barriers are likely to exhibit similar reputation scores. The pagerank algorithm may be relevant if we gather that the number and quality of links to news eventually cast something about their reputation score. Whilst those algorithms achieve a ranking effect, possible alternatives can be based on Naive bayes in a manner akin to spam filtering kernels. Although this is a sharper detection mechanism that will eventually exclude certain pieces of news if they fail the test, we can choose to employ it only at the beginning and then refine the set of failed data using the ranking algorithms. Decision trees can also be investigated and have a filtering effect. At the unsupervised level, we will investigate whether certain underlying patterns govern dishonest behaviour to which association rules in machine learning can be applied, and whether news sources relate to each other in general and to their environment, to which clustering algorithms can be applied. 


After running our model on unsupervised data, we aim to validate its accuracy and validity, thus paving the grounds for further model enhancements. Available options are numerous, and at this stage, it is not clear to the authors which of them will be most useful. With that said, we will be investigating all of the following evaluation models: Confusion Matrix, Gain and Lift Chart, Kolmogorov Smirnov Chart, AUC – ROC, Gini Coefficient, Concordant – Discordant Ratio, Root Mean Squared Error, and Cross Validation.
