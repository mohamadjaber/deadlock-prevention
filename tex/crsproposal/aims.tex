\section{Specific Aims}

This project is intended for one year. The main aims of this project as follows:

\begin{enumerate}
\item All of the below aims will make use of distributed, real-time streaming, analysis and visualisation as follows. Several frameworks exist for real time streaming. Two of the most notable ones are Apache Storm and Apache Spark, which offer distributed real-time streaming and processing capabilities. Spark is a fast and general engine for big data processing. Spark allows small-scale batching while Storm allows one-at-a-time processing model. The output of the streaming phase will be directed to another analytics framework, such as Spark, for further credibility analysis as detailed below. The output of some Storm bolts and of Spark will be stored in a non-relational database such as HBase that will be used to validate and annotate our analyser.
\item Collecting Data in English and Arabic, pulling it from different sources, such as social networks, news articles, and blogs. Some reports/data will originate from Facebook and Twitter users, as opposed to when users share sources from news or political organizations on those platforms. Our data will also be in different formats, such as text, videos, and MP3s. 

\item Cleaning and modeling the obtained data, which will be big and wide. Big data refers to data that has large volume. Wide data refers to data that contains many features and comes from many different sources. Here, we spot check the data for accuracy, making sure it is consistent in format, and free of duplications and other anomalies, so that it can eventually be put in meaningful form. In this step, we need to draw on expertise in computational linguistics to incorporate natural language processing (NLP) tools that can be applied on the streamed data, particularly, for tasks such as automated syntactic and semantic translation. Syntactic translation through machine translation models will be sought to resolve duplication and inconsistencies amongst articles in the same language. Semantic translation will be sought to resolve duplication and inconsistencies amongst articles in different languages. 
\item Analysing the news for dishonesty: in this step, we draw on expertise in media studies, to help develop qualitative detection mechanisms by which news articles are flagrantly cast as dishonest, with insight from previous experiences where news have indeed been deemed hishonest. Media bias is one of a number of factors that would predict with some likelihood whether reports are faked. Other factors may include the source of the report, the presence of news frames, or the platform of circulation. Further insight from media studies is required to advise whether a filtering or else a ranking approach to the problem fits best. This step should also incorporate a semi-supervised approach, where we learn from a small set of supervised (labeled) data and then carry on using a significantly larger unsupervised set. 
\item Running the model on unsupervised data, and validating our learning model, thus paving the grounds for further model enhancements, as we repeat the above step several times as needed to monitor how well previous mechanisms are working. We aim to release news based on strong, medium, and weak predictions of their honesty.
\item Visualisation mapping credible news to location: the output of Spark will be redirected to maps and dashboards that mimic critical stories in real-time. Interactive maps can be implemented using several open-source tools such as Leaflet or Shiny, which is an open-source JavaScript library for interactive maps. 
\end{enumerate}


